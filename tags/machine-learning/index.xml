<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Stars &amp; Sea</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Stars &amp; Sea</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML Summary Series (2) - Bias-Variance Tradeoff</title>
      <link>/2019/06/ml-summary-series-2---bias-variance-tradeoff/</link>
      <pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/ml-summary-series-2---bias-variance-tradeoff/</guid>
      <description>We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.</description>
    </item>
    
    <item>
      <title>ML Summary Series (1) - Concepts</title>
      <link>/2019/05/ml-summary-series-1---concepts/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/ml-summary-series-1---concepts/</guid>
      <description>Machine learning (ML) is by no means new to me. I took ML courses in college and in grad school. In college, I was also in a study group where we went through Bishopâ€™s PRML chapter by chapter. Later when I became a PhD, I use machine learning in plenty of projects, from the visualization of data after dimensional reduction through PCA, to building prediction models with logistic regression, random forest, support vector machines, etc.</description>
    </item>
    
  </channel>
</rss>