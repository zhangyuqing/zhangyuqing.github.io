<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.49.1 with theme Tranquilpeak 0.4.3-SNAPSHOT">
<meta name="author" content="Yuqing">
<meta name="keywords" content="">
<meta name="description" content="We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.">


<meta property="og:description" content="We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.">
<meta property="og:type" content="article">
<meta property="og:title" content="ML Summary Series (2) - Bias-variance tradeoff and learning theory basics">
<meta name="twitter:title" content="ML Summary Series (2) - Bias-variance tradeoff and learning theory basics">
<meta property="og:url" content="/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
<meta property="twitter:url" content="/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
<meta property="og:site_name" content="Stars &amp; Sea">
<meta property="og:description" content="We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.">
<meta name="twitter:description" content="We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2019-06-10T00:00:00">
  
  
    <meta property="article:modified_time" content="2019-06-10T00:00:00">
  
  
  
    
      <meta property="article:section" content="Toolbox">
    
  
  
    
      <meta property="article:tag" content="Machine learning">
    
      <meta property="article:tag" content="Study notes">
    
  


<meta name="twitter:card" content="summary">











  <meta property="og:image" content="/personal_files/profile.jpg">
  <meta property="twitter:image" content="/personal_files/profile.jpg">


    <title>ML Summary Series (2) - Bias-variance tradeoff and learning theory basics</title>

    <link rel="icon" href="/favicon.png">
    

    

    <link rel="canonical" href="/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="/css/style-ghokqi7s7hdo9hisbdrskvy84wnaw5cskdnpd4vwowlvtuiy1ejozrtvpzds.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">Stars &amp; Sea</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://zhangyuqing.github.io/">
    
    
    
      
        <img class="header-picture" src="/personal_files/profile.jpg" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      
        <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="/#about">
          <img class="sidebar-profile-picture" src="/personal_files/profile.jpg" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Yuqing</h4>
        
          <h5 class="sidebar-profile-bio">PhD Candidate in Bioinformatics</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/zhangyuqing">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/yuqing-zhang-272612a4">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://scholar.google.com/citations?user=lE6puDgAAAAJ&amp;hl=en&amp;authuser=1">
    
      <i class="sidebar-button-icon fa fa-lg fa-graduation-cap"></i>
      
      <span class="sidebar-button-desc">Google Scholar</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      
      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      ML Summary Series (2) - Bias-variance tradeoff and learning theory basics
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2019-06-10T00:00:00Z">
        
  June 10, 2019

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="/categories/toolbox">Toolbox</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.</p>
<div id="bias-variance-tradeoff" class="section level3">
<h3>Bias-variance tradeoff</h3>
<p>Before diving into the abstract learning theory, we start by a simple example of curve fitting. Suppose our data points (which is also called “examples”) fall on a 2-dimensional plain [Figure 1]. From this visualization, we see that <span class="math inline">\(y\)</span> may be associated with <span class="math inline">\(x\)</span>, and we could potentially use <span class="math inline">\(x\)</span> to predict <span class="math inline">\(y\)</span>. We use <span class="math inline">\(f\)</span> to denote the true association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(y=f(x)+\varepsilon\)</span>. Note that in real data, there is always going to be some level of noise in <span class="math inline">\(y\)</span>, and therefore, <span class="math inline">\(y\)</span> will not follow the trend of <span class="math inline">\(y=f(x)\)</span> exactly. So, although <span class="math inline">\(f(x)\)</span> is a scalar, we treat <span class="math inline">\(y\)</span> as a random variable with the compoenent of a normal distributed noise <span class="math inline">\(\varepsilon \sim N(0, \sigma^2)\)</span> to reflect the noise.</p>
<p>When a machine learning model comes in, it tries to learn an approximate of the true association, <span class="math inline">\(\hat{f}\)</span>. Then, when we apply the trained model on new data <span class="math inline">\(x&#39;\)</span> with the true label <span class="math inline">\(y&#39;\)</span> unknown, the apporximate is used to calculate the prediction: <span class="math inline">\(\hat{y}&#39; = \hat{f}(x&#39;)\)</span>. Now, suppose we use mean-squared-error (MSE) to measure the performance of the model. It can be derived that the MSE on new data can be decomposed into three components (which is the so-called “bias-variance tradeoff”):</p>
<p>[Insert figure]</p>
<p>Note that in the above decomposition, we made 2 assumptions: + Data in the training and the test set need to come from the same distribution. \ In other words, if we collect data from a data pool to train the model, where the true assocation is <span class="math inline">\(f\)</span>, then the new data need to be from the same pool with the same association. Of course, if the true association for new data is actually <span class="math inline">\(g\)</span>, which is inrelevant of <span class="math inline">\(f\)</span>, then we are very likely not going to make a good predction. + All examples should be independent. \ This is an assumption made by many classic models, and reflects the fact that the underlying association should be invariant of the data collected. If some examples in the data pool are correlated but some are not, then the association trained on correlated examples will be different from that on the uncorrelated ones. To sum up, in order to make meaningful predictions, we assume that training and test data are independent and identically distributed (i.i.d.).</p>
<p>Under these two assumptions, we can now make sense of the derivation. (2) to (3) comes from the assumption of identical distribution, meaning that the true label of the new example <span class="math inline">\(y&#39; = f(x&#39;) + \varepsilon&#39;, \varepsilon&#39; \sim N(0, \sigma^2)\)</span>. After expanding the squared term from (3) to (5), we use the independent assumption to get (6). Note that in this formation, x and f(x) are scalars, but <span class="math inline">\(\hat{f}\)</span> depends on the entire training set, including the noise in the training data. Therefore, both <span class="math inline">\(\hat{f}\)</span> and the prediction values <span class="math inline">\(\hat{f}(x&#39;)\)</span> should be seen as random variables. Because the noise in test data, <span class="math inline">\(\varepsilon&#39;\)</span>, and the trained model, <span class="math inline">\(\hat{f}\)</span>, are independent, the interaction term in (5), <span class="math inline">\(2E(\varepsilon&#39;(f(x&#39;) - \hat{f}(x&#39;)))\)</span>, equals 0. Finally, we use the formula for variance of any random variable to derive (7) from (6): <span class="math inline">\(var(X) = E(X^2) - (EX)^2\)</span>. The decomposition is derived with mean-squared-error measure in curve fitting, but it also generalizes to other models.</p>
<p>Generalization error, a measure for prediction performance on unseen data. This can be better illustrated with a simple curve fitting example. Suppose we want to use x to predict y for data points in [Figure 1]. Each point represents one example, with values of x and y. We use a mapping f to represent the target association. It is often the case in reality that data will not follow exactly on the trend defined by f, and there will always be some level of noise, large or small. Therefore, both y and x are random variables, and the target association is in the form <span class="math inline">\(y=f(x) + \varepsilon\)</span>, where <span class="math inline">\(\varepsilon∼N(0,σ^2)\)</span> is a random variable representing the noise.</p>
</div>
<div id="vc-dimension" class="section level3">
<h3>VC dimension</h3>
</div>
<div id="conclusions" class="section level3">
<h3>Conclusions</h3>
<p>I would close this post with a summary:</p>
<ul>
<li>Assumptions for data in supervised learning: training and test data are i.i.d.</li>
<li>Bias-variance tradeoff
<ul>
<li>Generalization error: prediction error of trained model on unseen data</li>
</ul></li>
</ul>

<ul>
<li>VC dimensions</li>
</ul>
</div>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="/tags/machine-learning/">Machine learning</a>

  <a class="tag tag--primary tag--small" href="/tags/study-notes/">Study notes</a>

                  </div>
                
              
            
            
              <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/ml-summary-series-1---concepts/" data-tooltip="ML Summary Series (1) - Concepts">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

              
                
                  <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
                
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2019 Yuqing. All Rights Reserved
  </span>
</footer>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
    
      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/ml-summary-series-1---concepts/" data-tooltip="ML Summary Series (1) - Concepts">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=%2F2019%2F06%2Fml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=%2F2019%2F06%2Fml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=%2F2019%2F06%2Fml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics%2F">
          <i class="fa fa-google-plus"></i><span>Share on Google&#43;</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=%2F2019%2F06%2Fml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics%2F">
          <i class="fa fa-weixin"></i><span>Share on WeChat</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="/personal_files/profile.jpg" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Yuqing</h4>
    
      <div id="about-card-bio">PhD Candidate in Bioinformatics</div>
    
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Boston &amp; SF Bay Area
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/06/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics/">
                <h3 class="media-heading">ML Summary Series (2) - Bias-variance tradeoff and learning theory basics</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Jun 6, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">We are all familiar with the workflow of supervised learning: fit models on training data, and make predictions on the test set. But why should performance of models in the training set tell us anything about that in the test set? Is model performance always generalizable to new data? Learning theory aims to address these questions under a general, abstract formulation of the supervised learning problem, without specifying details like the type of model or the source of data.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/05/ml-summary-series-1---concepts/">
                <h3 class="media-heading">ML Summary Series (1) - Concepts</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  May 5, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Machine learning (ML) is by no means new to me. I took ML courses in college and in grad school. In college, I was also in a study group where we went through Bishop’s PRML chapter by chapter. Later when I became a PhD, I use machine learning in plenty of projects, from the visualization of data after dimensional reduction through PCA, to building prediction models with logistic regression, random forest, support vector machines, etc.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         2 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('/images/cover.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="/js/script-p6llwctmalu8ncocgo5upocbylyeicglbexgxyreo826oslkc4fbyk7izxvf.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = '\/2019\/06\/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics\/';
          
            this.page.identifier = '\/2019\/06\/ml-summary-series-2---bias-variance-tradeoff-and-learning-theory-basics\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'zhangyuqing-github-io';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  




    
  </body>
</html>

